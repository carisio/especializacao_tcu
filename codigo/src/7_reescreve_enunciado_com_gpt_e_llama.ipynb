{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0195e380-7087-4fbe-b376-b78fc2efcf41",
   "metadata": {},
   "source": [
    "# Caderno 7 - Usa GPT e Llama para reescrever o enunciado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837a824-a564-42ff-a017-abcf20f68e40",
   "metadata": {},
   "source": [
    "## 1. Chaves de acesso e outras variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56cd7333-bcb4-4bcb-9184-61d4e43389c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "API Groq ········\n",
      "API OpenAI ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "GROQ_API = getpass(\"API Groq\")\n",
    "OPENAI_API = getpass(\"API OpenAI\")\n",
    "\n",
    "MODELO_LLAMA = \"llama3-70b-8192\"\n",
    "MODELO_GPT = \"gpt-3.5-turbo-0125\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f3c3c4-52c6-4136-bd5b-17e17fc94c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PASTA_DADOS = './dados/'\n",
    "PASTA_RESULTADO_CADERNO = f'{PASTA_DADOS}outputs/7_reescreve_enunciado_gpt_llama/'\n",
    "\n",
    "NOME_ARQUIVO_RESULTADO_LLAMA = f'{PASTA_RESULTADO_CADERNO}enunciados_reescritos_llama.pickle'\n",
    "NOME_ARQUIVO_RESULTADO_GPT = f'{PASTA_RESULTADO_CADERNO}enunciados_reescritos_gpt.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d872b504-9f1e-49d6-9376-66b897ea4862",
   "metadata": {},
   "source": [
    "## 2. Carrega os documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88074ddc-c172-4606-82a1-df6e83aef83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# A pasta dos JURIS aqui não é a pasta original, e sim o resultado do caderno 1 (os documentos já estão filtrados)\n",
    "PASTA_JURIS_TCU = f'{PASTA_DADOS}outputs/1_tratamento_juris_tcu/'\n",
    "\n",
    "# Carrega os arquivos \n",
    "def carrega_juris_tcu():\n",
    "    doc1 = pd.read_csv(f'{PASTA_JURIS_TCU}doc_tratado_parte_1.csv', sep='|')\n",
    "    doc2 = pd.read_csv(f'{PASTA_JURIS_TCU}doc_tratado_parte_2.csv', sep='|')\n",
    "    doc3 = pd.read_csv(f'{PASTA_JURIS_TCU}doc_tratado_parte_3.csv', sep='|')\n",
    "    doc4 = pd.read_csv(f'{PASTA_JURIS_TCU}doc_tratado_parte_4.csv', sep='|')\n",
    "    doc = pd.concat([doc1, doc2, doc3, doc4], ignore_index=True)\n",
    "    query = pd.read_csv(f'{PASTA_JURIS_TCU}query_tratado.csv', sep='|')\n",
    "    qrel = pd.read_csv(f'{PASTA_JURIS_TCU}qrel_tratado.csv', sep='|')\n",
    "\n",
    "    return doc, query, qrel\n",
    "\n",
    "docs, _, _ = carrega_juris_tcu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877e3c8-cb1a-4113-a39b-f0949d7c6d21",
   "metadata": {},
   "source": [
    "## 3. Define o prompt de sistema que será usado no GPT e no Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145e063e-d562-42f0-a21c-fa63c95fc233",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"Você é um especialista em sistemas de busca que usam o algoritmo BM25 e está trabalhando na indexação de uma base de dados de jurisprudência do Tribunal de Contas da União. Essa base está sofrendo com o problema de descasamento de vocabulário, ou seja, o usuário usa termos de pesquisa que não estão no enunciado da jurisprudência. Trata-se de um problema comum, pois o usuário não sabe como o enunciado está escrito.\n",
    "\n",
    "Para mitigar esse problema, além do enunciado original, será indexada uma versão reescrita do enunciado usando sinônimos. Dessa forma, espera-se que o usuário da pesquisa tenha maior probabilidade de encontrar o que procura.\n",
    "\n",
    "Sua tarefa é reescrever o enunciado. Procure sinônimos mais comuns para as palavras usadas no enunciado original. O público alvo da pesquisa é o cidadão em geral, com variados graus de instrução. Por isso, o enunciado deve ser reescrito de forma simplificada.\n",
    "\n",
    "Tudo o que você responder será indexado. Por isso, não forneça nada além da reescrita do enunciado.\n",
    "\"\"\"\n",
    "\n",
    "# Esse formato de mensagens é usado tanto na API da OpenAI quanto na API do Groq\n",
    "def get_messages(enunciado):\n",
    "    return [\n",
    "        { \"role\": \"system\", \"content\": system_message },\n",
    "        { \"role\": \"user\", \"content\": enunciado}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134860b2-d5df-470b-877f-3ce3fa2e8847",
   "metadata": {},
   "source": [
    "# 4. Define chamadas para GPT e Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3bb2c52-f71f-4263-ac4f-47eba797c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from openai import OpenAI\n",
    "\n",
    "clientGroq = Groq(api_key=GROQ_API)\n",
    "clientOpenAI = OpenAI(api_key=OPENAI_API)\n",
    "\n",
    "def completa_llama(enunciado):\n",
    "    response = clientGroq.chat.completions.create(\n",
    "        model=MODELO_LLAMA,\n",
    "        messages=get_messages(enunciado),\n",
    "        temperature=0,\n",
    "        max_tokens=1024,\n",
    "        top_p=1,\n",
    "        stream=False,\n",
    "        stop=None,\n",
    "    )    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def completa_gpt(enunciado):\n",
    "    response = clientOpenAI.chat.completions.create(\n",
    "        model=MODELO_GPT,\n",
    "        messages=get_messages(enunciado),\n",
    "        temperature=1,\n",
    "        max_tokens=1024,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1354fc2b-3584-4ab2-be00-1af1e0c54ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A invalidez permanente é incompatível com o exercício de qualquer cargo público, razão pela qual é indevida a acumulação de proventos de invalidez permanente com remuneração decorrente do exercício de outro cargo, cabendo restituição ao erário dos proventos recebidos durante a acumulação ilegal.\n",
      "....................................................................................................\n",
      "A incapacidade permanente impede o exercício de qualquer função pública, portanto, é injustificável receber ao mesmo tempo benefícios por incapacidade permanente e salário de outro cargo, devendo ser devolvidos ao Estado os valores recebidos durante a acumulação irregular.\n",
      "....................................................................................................\n",
      "A incapacidade permanente não permite trabalhar em cargos públicos, por isso não é correto acumular a aposentadoria por invalidez com o salário de outro cargo. Os valores recebidos de forma irregular devem ser devolvidos ao governo.\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 3.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Testa com um enunciado qualquer pra ver se as chamadas estão funcionando:\n",
    "enunciado = \"A invalidez permanente é incompatível com o exercício de qualquer cargo público, razão pela qual é indevida a acumulação de proventos de invalidez permanente com remuneração decorrente do exercício de outro cargo, cabendo restituição ao erário dos proventos recebidos durante a acumulação ilegal.\"\n",
    "print(enunciado)\n",
    "print('.'*100)\n",
    "print(completa_llama(enunciado))\n",
    "print('.'*100)\n",
    "print(completa_gpt(enunciado))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac4ff44-0da0-4a75-b954-03ae1e875905",
   "metadata": {},
   "source": [
    "## 5. Executa as chamadas ao Llama e GPT para toda a base de dados\n",
    "\n",
    "As chamadas ao Groq estão gratuitas, mas tem limite de tempo. Para não estourar o tempo, vou garantir que cada execução do loop dure pelo menos 10 segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69cfb511-d427-4b77-ad72-8dd4a0f14944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados do Llama recuperados\n",
      "Resultados do GPT recuperados\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Objeto que guardarão a reescrita dos enunciados por documento\n",
    "enunciado_llama_por_doc = {}\n",
    "enunciado_gpt_por_doc = {}\n",
    "\n",
    "# Verifica se os arquivos já existem. Se já existem, recupera.\n",
    "if os.path.exists(NOME_ARQUIVO_RESULTADO_LLAMA):\n",
    "    with open(NOME_ARQUIVO_RESULTADO_LLAMA, 'rb') as f:\n",
    "        enunciado_llama_por_doc = pickle.load(f)\n",
    "    print(\"Resultados do Llama recuperados\")\n",
    "\n",
    "if os.path.exists(NOME_ARQUIVO_RESULTADO_GPT):\n",
    "    with open(NOME_ARQUIVO_RESULTADO_GPT, 'rb') as f:\n",
    "        enunciado_gpt_por_doc = pickle.load(f)\n",
    "    print(\"Resultados do GPT recuperados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81119c59-9edf-488d-a83d-5ad626798e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 16045/16045 [45:12:24<00:00, 10.14s/it]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def remove_html(html):\n",
    "    return re.sub(\"<[^>]*>\", \"\", html).strip()\n",
    "\n",
    "# Percorre o dataframe de ocumentos e gera as queries\n",
    "for i, row in tqdm(docs.iterrows(), total=len(docs)):\n",
    "    # Começa a guardar o tempo\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extrai a key e o enunciado\n",
    "    doc_key = row.KEY\n",
    "    enunciado = remove_html(row.ENUNCIADO)\n",
    "    \n",
    "    # Se já tem o enunciado gerado, passa para o próximo\n",
    "    if doc_key in enunciado_llama_por_doc.keys() and doc_key in enunciado_gpt_por_doc.keys():\n",
    "        continue\n",
    "           \n",
    "    # Gera versões alternativas do enunciado\n",
    "    enunciado_llama = completa_llama(enunciado)\n",
    "    enunciado_gpt = completa_gpt(enunciado)\n",
    "    \n",
    "    # Salva no mapa\n",
    "    enunciado_llama_por_doc[doc_key] = enunciado_llama\n",
    "    enunciado_gpt_por_doc[doc_key] = enunciado_gpt\n",
    "    \n",
    "    # Salva em arquivos pickle\n",
    "    with open(NOME_ARQUIVO_RESULTADO_LLAMA, 'wb') as f:\n",
    "        pickle.dump(enunciado_llama_por_doc, f)\n",
    "    \n",
    "    with open(NOME_ARQUIVO_RESULTADO_GPT, 'wb') as f:\n",
    "        pickle.dump(enunciado_gpt_por_doc, f)\n",
    "\n",
    "    # Mede o tempo transcorrido\n",
    "    elapsed_time = time.time() - start_time\n",
    "    # Faz um sleep se for necessário\n",
    "    if elapsed_time < 10:\n",
    "        time.sleep(10 - elapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
